{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using GPU\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"You are using GPU\")\n",
    "else:\n",
    "    print(\"You are using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 799\n",
      "Validation dataset size: 200\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data_processing/csv_file/1hand_25_9_0.csv')\n",
    "\n",
    "# Define your PyTorch dataset class\n",
    "class ProcessData(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe.iloc[:, 1:].values\n",
    "        self.labels = dataframe.iloc[:, 0].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\n",
    "            'data': torch.tensor(self.data[idx], dtype=torch.float32, device=device),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long, device=device)\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "keypoint_dataset = ProcessData(df)\n",
    "\n",
    "train_size = int(0.8 * len(keypoint_dataset))\n",
    "val_size = len(keypoint_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(keypoint_dataset, [train_size, val_size])\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"Training dataset size:\", len(train_dataset))\n",
    "print(\"Validation dataset size:\", len(val_dataset))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "fc1.weight \t torch.Size([40, 63])\n",
      "fc1.bias \t torch.Size([40])\n",
      "fc2.weight \t torch.Size([20, 40])\n",
      "fc2.bias \t torch.Size([20])\n",
      "fc3.weight \t torch.Size([10, 20])\n",
      "fc3.bias \t torch.Size([10])\n",
      "fc4.weight \t torch.Size([5, 10])\n",
      "fc4.bias \t torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "import model as m\n",
    "model = m.SimpleNN2().to(device)\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "dataloader = DataLoader(keypoint_dataset, batch_size = 128, shuffle = True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Training Loss: 1.6209\n",
      "Epoch [2/50], Training Loss: 1.6030\n",
      "Epoch [3/50], Training Loss: 1.6179\n",
      "Epoch [4/50], Training Loss: 1.5965\n",
      "Epoch [5/50], Training Loss: 1.5515\n",
      "Epoch [6/50], Training Loss: 1.5488\n",
      "Epoch [7/50], Training Loss: 1.5341\n",
      "Epoch [8/50], Training Loss: 1.4845\n",
      "Epoch [9/50], Training Loss: 1.4319\n",
      "Epoch [10/50], Training Loss: 1.4614\n",
      "Validation Loss: 1.3123, Training Accuracy: 54.07%, Validation Accuracy: 81.50%\n",
      "Epoch [11/50], Training Loss: 1.2817\n",
      "Epoch [12/50], Training Loss: 1.3092\n",
      "Epoch [13/50], Training Loss: 1.3824\n",
      "Epoch [14/50], Training Loss: 1.3456\n",
      "Epoch [15/50], Training Loss: 1.3367\n",
      "Epoch [16/50], Training Loss: 1.2749\n",
      "Epoch [17/50], Training Loss: 1.2609\n",
      "Epoch [18/50], Training Loss: 1.2103\n",
      "Epoch [19/50], Training Loss: 1.2004\n",
      "Epoch [20/50], Training Loss: 1.2212\n",
      "Validation Loss: 1.0083, Training Accuracy: 64.96%, Validation Accuracy: 95.50%\n",
      "Epoch [21/50], Training Loss: 0.9418\n",
      "Epoch [22/50], Training Loss: 1.0987\n",
      "Epoch [23/50], Training Loss: 1.3509\n",
      "Epoch [24/50], Training Loss: 1.3305\n",
      "Epoch [25/50], Training Loss: 1.2939\n",
      "Epoch [26/50], Training Loss: 1.0582\n",
      "Epoch [27/50], Training Loss: 1.2541\n",
      "Epoch [28/50], Training Loss: 1.0924\n",
      "Epoch [29/50], Training Loss: 1.2580\n",
      "Epoch [30/50], Training Loss: 1.1427\n",
      "Validation Loss: 0.9206, Training Accuracy: 72.97%, Validation Accuracy: 100.00%\n",
      "Epoch [31/50], Training Loss: 0.9101\n",
      "Epoch [32/50], Training Loss: 1.1845\n",
      "Epoch [33/50], Training Loss: 1.1869\n",
      "Epoch [34/50], Training Loss: 1.1497\n",
      "Epoch [35/50], Training Loss: 1.2673\n",
      "Epoch [36/50], Training Loss: 1.1834\n",
      "Epoch [37/50], Training Loss: 1.1536\n",
      "Epoch [38/50], Training Loss: 1.1515\n",
      "Epoch [39/50], Training Loss: 1.0534\n",
      "Epoch [40/50], Training Loss: 1.1333\n",
      "Validation Loss: 0.9142, Training Accuracy: 76.35%, Validation Accuracy: 100.00%\n",
      "Epoch [41/50], Training Loss: 0.9092\n",
      "Epoch [42/50], Training Loss: 1.1255\n",
      "Epoch [43/50], Training Loss: 1.2419\n",
      "Epoch [44/50], Training Loss: 1.1664\n",
      "Epoch [45/50], Training Loss: 1.0260\n",
      "Epoch [46/50], Training Loss: 1.1028\n",
      "Epoch [47/50], Training Loss: 1.0079\n",
      "Epoch [48/50], Training Loss: 1.0792\n",
      "Epoch [49/50], Training Loss: 1.1451\n",
      "Epoch [50/50], Training Loss: 1.0437\n",
      "Validation Loss: 0.9087, Training Accuracy: 77.10%, Validation Accuracy: 100.00%\n",
      "Epoch [51/50], Training Loss: 0.9093\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "previous_loss = float('inf')  # Initialize with a high value\n",
    "writer = SummaryWriter()\n",
    "val_loss = 0.0\n",
    "for epoch in range(num_epochs+1):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    if epoch != 0 and epoch % 10 == 0:\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_total_correct = 0\n",
    "        val_total_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = batch['data']\n",
    "                labels = batch['label']\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs, labels)\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total_samples += labels.size(0)\n",
    "                val_total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * val_total_correct / val_total_samples\n",
    "        print(f'Validation Loss: {val_loss.item():.4f}, Training Accuracy: {train_accuracy:.2f}%, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "        writer.add_scalar('Validation Loss', val_loss.item(), epoch)\n",
    "        writer.add_scalar('Validation Accuracy', val_accuracy, epoch)\n",
    "    for batch in train_loader:\n",
    "        inputs = batch['data']\n",
    "        labels = batch['label']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_accuracy = 100 * total_correct / total_samples\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item():.4f}')\n",
    "\n",
    "    writer.add_scalar('Training Loss', loss.item(), epoch)\n",
    "    writer.add_scalar('Training Accuracy', train_accuracy, epoch)\n",
    "    if val_loss < previous_loss:\n",
    "        previous_loss = val_loss  # Update the previous loss\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"models/test.pth\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run this in terminal to open tensorboard: tensorboard --logdir=runs\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
